{
  "url": "https://docs.python.org/3/library/tokenize.html",
  "status_code": 200,
  "depth": 1,
  "referrer": "https://docs.python.org/3/library/",
  "raw_html": "<!DOCTYPE html>\n\n<html lang=\"en\" data-content_root=\"../\">\n  <head>\n    <meta charset=\"utf-8\" />\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" /><meta name=\"viewport\" content=\"width=device-width, initial-scale=1\" />\n<meta property=\"og:title\" content=\"tokenize — Tokenizer for Python source\" />\n<meta property=\"og:type\" content=\"website\" />\n<meta property=\"og:url\" content=\"https://docs.python.org/3/library/tokenize.html\" />\n<meta property=\"og:site_name\" content=\"Python documentation\" />\n<meta property=\"og:description\" content=\"Source code: Lib/tokenize.py The tokenize module provides a lexical scanner for Python source code, implemented in Python. The scanner in this module returns comments as tokens as well, making it u...\" />\n<meta property=\"og:image:width\" content=\"1146\" />\n<meta property=\"og:image:height\" content=\"600\" />\n<meta property=\"og:image\" content=\"https://docs.python.org/3.14/_images/social_previews/summary_library_tokenize_98863b6b.png\" />\n<meta property=\"og:image:alt\" content=\"Source code: Lib/tokenize.py The tokenize module provides a lexical scanner for Python source code, implemented in Python. The scanner in this module returns comments as tokens as well, making it u...\" />\n<meta name=\"description\" content=\"Source code: Lib/tokenize.py The tokenize module provides a lexical scanner for Python source code, implemented in Python. The scanner in this module returns comments as tokens as well, making it u...\" />\n<meta name=\"twitter:card\" content=\"summary_large_image\" />\n<meta name=\"theme-color\" content=\"#3776ab\">\n\n    <title>tokenize — Tokenizer for Python source &#8212; Python 3.14.0 documentation</title><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    \n    <link rel=\"stylesheet\" type=\"text/css\" href=\"../_static/pygments.css?v=b86133f3\" />\n    <link rel=\"stylesheet\" type=\"text/css\" href=\"../_static/classic.css?v=234b1a7c\" />\n    <link rel=\"stylesheet\" type=\"text/css\" href=\"../_static/pydoctheme.css?v=8cd84f99\" />\n    <link id=\"pygments_dark_css\" media=\"(prefers-color-scheme: dark)\" rel=\"stylesheet\" type=\"text/css\" href=\"../_static/pygments_dark.css?v=5349f25f\" />\n    \n    <script src=\"../_static/documentation_options.js?v=e4f4b189\"></script>\n    <script src=\"../_static/doctools.js?v=9bcbadda\"></script>\n    <script src=\"../_static/sphinx_highlight.js?v=dc90522c\"></script>\n    \n    <script src=\"../_static/sidebar.js\"></script>\n    \n    <link rel=\"search\" type=\"application/opensearchdescription+xml\"\n          title=\"Search within Python 3.14.0 documentation\"\n          href=\"../_static/opensearch.xml\"/>\n    <link rel=\"author\" title=\"About these documents\" href=\"../about.html\" />\n    <link rel=\"index\" title=\"Index\" href=\"../genindex.html\" />\n    <link rel=\"search\" title=\"Search\" href=\"../search.html\" />\n    <link rel=\"copyright\" title=\"Copyright\" href=\"../copyright.html\" />\n    <link rel=\"next\" title=\"tabnanny — Detection of ambiguous indentation\" href=\"tabnanny.html\" />\n    <link rel=\"prev\" title=\"keyword — Testing for Python keywords\" href=\"keyword.html\" />\n    \n      \n      <script defer file-types=\"bz2,epub,zip\" data-domain=\"docs.python.org\" src=\"https://analytics.python.org/js/script.file-downloads.outbound-links.js\"></script>\n      \n      <link rel=\"canonical\" href=\"https://docs.python.org/3/library/tokenize.html\">\n      \n    \n\n    \n    <style>\n      @media only screen {\n        table.full-width-table {\n            width: 100%;\n        }\n      }\n    </style>\n<link rel=\"stylesheet\" href=\"../_static/pydoctheme_dark.css\" media=\"(prefers-color-scheme: dark)\" id=\"pydoctheme_dark_css\">\n    <link rel=\"shortcut icon\" type=\"image/png\" href=\"../_static/py.svg\">\n            <script type=\"text/javascript\" src=\"../_static/copybutton.js\"></script>\n            <script type=\"text/javascript\" src=\"../_static/menu.js\"></script>\n            <script type=\"text/javascript\" src=\"../_static/search-focus.js\"></script>\n            <script type=\"text/javascript\" src=\"../_static/themetoggle.js\"></script> \n            <script type=\"text/javascript\" src=\"../_static/rtd_switcher.js\"></script>\n            <meta name=\"readthedocs-addons-api-version\" content=\"1\">\n\n  </head>\n<body>\n<div class=\"mobile-nav\">\n    <input type=\"checkbox\" id=\"menuToggler\" class=\"toggler__input\" aria-controls=\"navigation\"\n           aria-pressed=\"false\" aria-expanded=\"false\" role=\"button\" aria-label=\"Menu\">\n    <nav class=\"nav-content\" role=\"navigation\">\n        <label for=\"menuToggler\" class=\"toggler__label\">\n            <span></span>\n        </label>\n        <span class=\"nav-items-wrapper\">\n            <a href=\"https://www.python.org/\" class=\"nav-logo\">\n                <img src=\"../_static/py.svg\" alt=\"Python logo\">\n            </a>\n            <span class=\"version_switcher_placeholder\"></span>\n            <form role=\"search\" class=\"search\" action=\"../search.html\" method=\"get\">\n                <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" class=\"search-icon\">\n                    <path fill-rule=\"nonzero\" fill=\"currentColor\" d=\"M15.5 14h-.79l-.28-.27a6.5 6.5 0 001.48-5.34c-.47-2.78-2.79-5-5.59-5.34a6.505 6.505 0 00-7.27 7.27c.34 2.8 2.56 5.12 5.34 5.59a6.5 6.5 0 005.34-1.48l.27.28v.79l4.25 4.25c.41.41 1.08.41 1.49 0 .41-.41.41-1.08 0-1.49L15.5 14zm-6 0C7.01 14 5 11.99 5 9.5S7.01 5 9.5 5 14 7.01 14 9.5 11.99 14 9.5 14z\"></path>\n                </svg>\n                <input placeholder=\"Quick search\" aria-label=\"Quick search\" type=\"search\" name=\"q\">\n                <input type=\"submit\" value=\"Go\">\n            </form>\n        </span>\n    </nav>\n    <div class=\"menu-wrapper\">\n        <nav class=\"menu\" role=\"navigation\" aria-label=\"main navigation\">\n            <div class=\"language_switcher_placeholder\"></div>\n            \n<label class=\"theme-selector-label\">\n    Theme\n    <select class=\"theme-selector\" oninput=\"activateTheme(this.value)\">\n        <option value=\"auto\" selected>Auto</option>\n        <option value=\"light\">Light</option>\n        <option value=\"dark\">Dark</option>\n    </select>\n</label>\n  <div>\n    <h3><a href=\"../contents.html\">Table of Contents</a></h3>\n    <ul>\n<li><a class=\"reference internal\" href=\"#\"><code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">tokenize</span></code> — Tokenizer for Python source</a><ul>\n<li><a class=\"reference internal\" href=\"#tokenizing-input\">Tokenizing Input</a></li>\n<li><a class=\"reference internal\" href=\"#command-line-usage\">Command-Line Usage</a></li>\n<li><a class=\"reference internal\" href=\"#examples\">Examples</a></li>\n</ul>\n</li>\n</ul>\n\n  </div>\n  <div>\n    <h4>Previous topic</h4>\n    <p class=\"topless\"><a href=\"keyword.html\"\n                          title=\"previous chapter\"><code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">keyword</span></code> — Testing for Python keywords</a></p>\n  </div>\n  <div>\n    <h4>Next topic</h4>\n    <p class=\"topless\"><a href=\"tabnanny.html\"\n                          title=\"next chapter\"><code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">tabnanny</span></code> — Detection of ambiguous indentation</a></p>\n  </div>\n  <div role=\"note\" aria-label=\"source link\">\n    <h3>This page</h3>\n    <ul class=\"this-page-menu\">\n      <li><a href=\"../bugs.html\">Report a bug</a></li>\n      <li>\n        <a href=\"https://github.com/python/cpython/blob/main/Doc/library/tokenize.rst?plain=1\"\n            rel=\"nofollow\">Show source\n        </a>\n      </li>\n    </ul>\n  </div>\n        </nav>\n    </div>\n</div>\n\n  \n    <div class=\"related\" role=\"navigation\" aria-label=\"Related\">\n      <h3>Navigation</h3>\n      <ul>\n        <li class=\"right\" style=\"margin-right: 10px\">\n          <a href=\"../genindex.html\" title=\"General Index\"\n             accesskey=\"I\">index</a></li>\n        <li class=\"right\" >\n          <a href=\"../py-modindex.html\" title=\"Python Module Index\"\n             >modules</a> |</li>\n        <li class=\"right\" >\n          <a href=\"tabnanny.html\" title=\"tabnanny — Detection of ambiguous indentation\"\n             accesskey=\"N\">next</a> |</li>\n        <li class=\"right\" >\n          <a href=\"keyword.html\" title=\"keyword — Testing for Python keywords\"\n             accesskey=\"P\">previous</a> |</li>\n\n          <li><img src=\"../_static/py.svg\" alt=\"Python logo\" style=\"vertical-align: middle; margin-top: -1px\"></li>\n          <li><a href=\"https://www.python.org/\">Python</a> &#187;</li>\n          <li class=\"switchers\">\n            <div class=\"language_switcher_placeholder\"></div>\n            <div class=\"version_switcher_placeholder\"></div>\n          </li>\n          <li>\n              \n          </li>\n    <li id=\"cpython-language-and-version\">\n      <a href=\"../index.html\">3.14.0 Documentation</a> &#187;\n    </li>\n\n          <li class=\"nav-item nav-item-1\"><a href=\"index.html\" >The Python Standard Library</a> &#187;</li>\n          <li class=\"nav-item nav-item-2\"><a href=\"language.html\" accesskey=\"U\">Python Language Services</a> &#187;</li>\n        <li class=\"nav-item nav-item-this\"><a href=\"\"><code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">tokenize</span></code> — Tokenizer for Python source</a></li>\n                <li class=\"right\">\n                    \n\n    <div class=\"inline-search\" role=\"search\">\n        <form class=\"inline-search\" action=\"../search.html\" method=\"get\">\n          <input placeholder=\"Quick search\" aria-label=\"Quick search\" type=\"search\" name=\"q\" id=\"search-box\">\n          <input type=\"submit\" value=\"Go\">\n        </form>\n    </div>\n                     |\n                </li>\n            <li class=\"right\">\n<label class=\"theme-selector-label\">\n    Theme\n    <select class=\"theme-selector\" oninput=\"activateTheme(this.value)\">\n        <option value=\"auto\" selected>Auto</option>\n        <option value=\"light\">Light</option>\n        <option value=\"dark\">Dark</option>\n    </select>\n</label> |</li>\n            \n      </ul>\n    </div>    \n\n    <div class=\"document\">\n      <div class=\"documentwrapper\">\n        <div class=\"bodywrapper\">\n          <div class=\"body\" role=\"main\">\n            \n  <section id=\"module-tokenize\">\n<span id=\"tokenize-tokenizer-for-python-source\"></span><h1><code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">tokenize</span></code> — Tokenizer for Python source<a class=\"headerlink\" href=\"#module-tokenize\" title=\"Link to this heading\">¶</a></h1>\n<p><strong>Source code:</strong> <a class=\"extlink-source reference external\" href=\"https://github.com/python/cpython/tree/3.14/Lib/tokenize.py\">Lib/tokenize.py</a></p>\n<hr class=\"docutils\" />\n<p>The <a class=\"reference internal\" href=\"#module-tokenize\" title=\"tokenize: Lexical scanner for Python source code.\"><code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">tokenize</span></code></a> module provides a lexical scanner for Python source code,\nimplemented in Python.  The scanner in this module returns comments as tokens\nas well, making it useful for implementing “pretty-printers”, including\ncolorizers for on-screen displays.</p>\n<p>To simplify token stream handling, all <a class=\"reference internal\" href=\"../reference/lexical_analysis.html#operators\"><span class=\"std std-ref\">operator</span></a> and\n<a class=\"reference internal\" href=\"../reference/lexical_analysis.html#delimiters\"><span class=\"std std-ref\">delimiter</span></a> tokens and <a class=\"reference internal\" href=\"constants.html#Ellipsis\" title=\"Ellipsis\"><code class=\"xref py py-data docutils literal notranslate\"><span class=\"pre\">Ellipsis</span></code></a> are returned using\nthe generic <a class=\"reference internal\" href=\"token.html#token.OP\" title=\"token.OP\"><code class=\"xref py py-data docutils literal notranslate\"><span class=\"pre\">OP</span></code></a> token type.  The exact\ntype can be determined by checking the <code class=\"docutils literal notranslate\"><span class=\"pre\">exact_type</span></code> property on the\n<a class=\"reference internal\" href=\"../glossary.html#term-named-tuple\"><span class=\"xref std std-term\">named tuple</span></a> returned from <a class=\"reference internal\" href=\"#tokenize.tokenize\" title=\"tokenize.tokenize\"><code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">tokenize.tokenize()</span></code></a>.</p>\n<div class=\"admonition warning\">\n<p class=\"admonition-title\">Warning</p>\n<p>Note that the functions in this module are only designed to parse\nsyntactically valid Python code (code that does not raise when parsed\nusing <a class=\"reference internal\" href=\"ast.html#ast.parse\" title=\"ast.parse\"><code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">ast.parse()</span></code></a>).  The behavior of the functions in this module is\n<strong>undefined</strong> when providing invalid Python code and it can change at any\npoint.</p>\n</div>\n<section id=\"tokenizing-input\">\n<h2>Tokenizing Input<a class=\"headerlink\" href=\"#tokenizing-input\" title=\"Link to this heading\">¶</a></h2>\n<p>The primary entry point is a <a class=\"reference internal\" href=\"../glossary.html#term-generator\"><span class=\"xref std std-term\">generator</span></a>:</p>\n<dl class=\"py function\">\n<dt class=\"sig sig-object py\" id=\"tokenize.tokenize\">\n<span class=\"sig-prename descclassname\"><span class=\"pre\">tokenize.</span></span><span class=\"sig-name descname\"><span class=\"pre\">tokenize</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">readline</span></span></em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#tokenize.tokenize\" title=\"Link to this definition\">¶</a></dt>\n<dd><p>The <a class=\"reference internal\" href=\"#tokenize.tokenize\" title=\"tokenize.tokenize\"><code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">tokenize()</span></code></a> generator requires one argument, <em>readline</em>, which\nmust be a callable object which provides the same interface as the\n<a class=\"reference internal\" href=\"io.html#io.IOBase.readline\" title=\"io.IOBase.readline\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">io.IOBase.readline()</span></code></a> method of file objects.  Each call to the\nfunction should return one line of input as bytes.</p>\n<p>The generator produces 5-tuples with these members: the token type; the\ntoken string; a 2-tuple <code class=\"docutils literal notranslate\"><span class=\"pre\">(srow,</span> <span class=\"pre\">scol)</span></code> of ints specifying the row and\ncolumn where the token begins in the source; a 2-tuple <code class=\"docutils literal notranslate\"><span class=\"pre\">(erow,</span> <span class=\"pre\">ecol)</span></code> of\nints specifying the row and column where the token ends in the source; and\nthe line on which the token was found. The line passed (the last tuple item)\nis the <em>physical</em> line.  The 5 tuple is returned as a <a class=\"reference internal\" href=\"../glossary.html#term-named-tuple\"><span class=\"xref std std-term\">named tuple</span></a>\nwith the field names:\n<code class=\"docutils literal notranslate\"><span class=\"pre\">type</span> <span class=\"pre\">string</span> <span class=\"pre\">start</span> <span class=\"pre\">end</span> <span class=\"pre\">line</span></code>.</p>\n<p>The returned <a class=\"reference internal\" href=\"../glossary.html#term-named-tuple\"><span class=\"xref std std-term\">named tuple</span></a> has an additional property named\n<code class=\"docutils literal notranslate\"><span class=\"pre\">exact_type</span></code> that contains the exact operator type for\n<a class=\"reference internal\" href=\"token.html#token.OP\" title=\"token.OP\"><code class=\"xref py py-data docutils literal notranslate\"><span class=\"pre\">OP</span></code></a> tokens.  For all other token types <code class=\"docutils literal notranslate\"><span class=\"pre\">exact_type</span></code>\nequals the named tuple <code class=\"docutils literal notranslate\"><span class=\"pre\">type</span></code> field.</p>\n<div class=\"versionchanged\">\n<p><span class=\"versionmodified changed\">Changed in version 3.1: </span>Added support for named tuples.</p>\n</div>\n<div class=\"versionchanged\">\n<p><span class=\"versionmodified changed\">Changed in version 3.3: </span>Added support for <code class=\"docutils literal notranslate\"><span class=\"pre\">exact_type</span></code>.</p>\n</div>\n<p><a class=\"reference internal\" href=\"#tokenize.tokenize\" title=\"tokenize.tokenize\"><code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">tokenize()</span></code></a> determines the source encoding of the file by looking for a\nUTF-8 BOM or encoding cookie, according to <span class=\"target\" id=\"index-0\"></span><a class=\"pep reference external\" href=\"https://peps.python.org/pep-0263/\"><strong>PEP 263</strong></a>.</p>\n</dd></dl>\n\n<dl class=\"py function\">\n<dt class=\"sig sig-object py\" id=\"tokenize.generate_tokens\">\n<span class=\"sig-prename descclassname\"><span class=\"pre\">tokenize.</span></span><span class=\"sig-name descname\"><span class=\"pre\">generate_tokens</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">readline</span></span></em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#tokenize.generate_tokens\" title=\"Link to this definition\">¶</a></dt>\n<dd><p>Tokenize a source reading unicode strings instead of bytes.</p>\n<p>Like <a class=\"reference internal\" href=\"#tokenize.tokenize\" title=\"tokenize.tokenize\"><code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">tokenize()</span></code></a>, the <em>readline</em> argument is a callable returning\na single line of input. However, <a class=\"reference internal\" href=\"#tokenize.generate_tokens\" title=\"tokenize.generate_tokens\"><code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">generate_tokens()</span></code></a> expects <em>readline</em>\nto return a str object rather than bytes.</p>\n<p>The result is an iterator yielding named tuples, exactly like\n<a class=\"reference internal\" href=\"#tokenize.tokenize\" title=\"tokenize.tokenize\"><code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">tokenize()</span></code></a>. It does not yield an <a class=\"reference internal\" href=\"token.html#token.ENCODING\" title=\"token.ENCODING\"><code class=\"xref py py-data docutils literal notranslate\"><span class=\"pre\">ENCODING</span></code></a> token.</p>\n</dd></dl>\n\n<p>All constants from the <a class=\"reference internal\" href=\"token.html#module-token\" title=\"token: Constants representing terminal nodes of the parse tree.\"><code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">token</span></code></a> module are also exported from\n<a class=\"reference internal\" href=\"#module-tokenize\" title=\"tokenize: Lexical scanner for Python source code.\"><code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">tokenize</span></code></a>.</p>\n<p>Another function is provided to reverse the tokenization process. This is\nuseful for creating tools that tokenize a script, modify the token stream, and\nwrite back the modified script.</p>\n<dl class=\"py function\">\n<dt class=\"sig sig-object py\" id=\"tokenize.untokenize\">\n<span class=\"sig-prename descclassname\"><span class=\"pre\">tokenize.</span></span><span class=\"sig-name descname\"><span class=\"pre\">untokenize</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">iterable</span></span></em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#tokenize.untokenize\" title=\"Link to this definition\">¶</a></dt>\n<dd><p>Converts tokens back into Python source code.  The <em>iterable</em> must return\nsequences with at least two elements, the token type and the token string.\nAny additional sequence elements are ignored.</p>\n<p>The result is guaranteed to tokenize back to match the input so that the\nconversion is lossless and round-trips are assured.  The guarantee applies\nonly to the token type and token string as the spacing between tokens\n(column positions) may change.</p>\n<p>It returns bytes, encoded using the <a class=\"reference internal\" href=\"token.html#token.ENCODING\" title=\"token.ENCODING\"><code class=\"xref py py-data docutils literal notranslate\"><span class=\"pre\">ENCODING</span></code></a> token, which\nis the first token sequence output by <a class=\"reference internal\" href=\"#tokenize.tokenize\" title=\"tokenize.tokenize\"><code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">tokenize()</span></code></a>. If there is no\nencoding token in the input, it returns a str instead.</p>\n</dd></dl>\n\n<p><a class=\"reference internal\" href=\"#tokenize.tokenize\" title=\"tokenize.tokenize\"><code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">tokenize()</span></code></a> needs to detect the encoding of source files it tokenizes. The\nfunction it uses to do this is available:</p>\n<dl class=\"py function\">\n<dt class=\"sig sig-object py\" id=\"tokenize.detect_encoding\">\n<span class=\"sig-prename descclassname\"><span class=\"pre\">tokenize.</span></span><span class=\"sig-name descname\"><span class=\"pre\">detect_encoding</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">readline</span></span></em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#tokenize.detect_encoding\" title=\"Link to this definition\">¶</a></dt>\n<dd><p>The <a class=\"reference internal\" href=\"#tokenize.detect_encoding\" title=\"tokenize.detect_encoding\"><code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">detect_encoding()</span></code></a> function is used to detect the encoding that\nshould be used to decode a Python source file. It requires one argument,\nreadline, in the same way as the <a class=\"reference internal\" href=\"#tokenize.tokenize\" title=\"tokenize.tokenize\"><code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">tokenize()</span></code></a> generator.</p>\n<p>It will call readline a maximum of twice, and return the encoding used\n(as a string) and a list of any lines (not decoded from bytes) it has read\nin.</p>\n<p>It detects the encoding from the presence of a UTF-8 BOM or an encoding\ncookie as specified in <span class=\"target\" id=\"index-1\"></span><a class=\"pep reference external\" href=\"https://peps.python.org/pep-0263/\"><strong>PEP 263</strong></a>. If both a BOM and a cookie are present,\nbut disagree, a <a class=\"reference internal\" href=\"exceptions.html#SyntaxError\" title=\"SyntaxError\"><code class=\"xref py py-exc docutils literal notranslate\"><span class=\"pre\">SyntaxError</span></code></a> will be raised. Note that if the BOM is found,\n<code class=\"docutils literal notranslate\"><span class=\"pre\">'utf-8-sig'</span></code> will be returned as an encoding.</p>\n<p>If no encoding is specified, then the default of <code class=\"docutils literal notranslate\"><span class=\"pre\">'utf-8'</span></code> will be\nreturned.</p>\n<p>Use <a class=\"reference internal\" href=\"#tokenize.open\" title=\"tokenize.open\"><code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">open()</span></code></a> to open Python source files: it uses\n<a class=\"reference internal\" href=\"#tokenize.detect_encoding\" title=\"tokenize.detect_encoding\"><code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">detect_encoding()</span></code></a> to detect the file encoding.</p>\n</dd></dl>\n\n<dl class=\"py function\">\n<dt class=\"sig sig-object py\" id=\"tokenize.open\">\n<span class=\"sig-prename descclassname\"><span class=\"pre\">tokenize.</span></span><span class=\"sig-name descname\"><span class=\"pre\">open</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">filename</span></span></em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#tokenize.open\" title=\"Link to this definition\">¶</a></dt>\n<dd><p>Open a file in read only mode using the encoding detected by\n<a class=\"reference internal\" href=\"#tokenize.detect_encoding\" title=\"tokenize.detect_encoding\"><code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">detect_encoding()</span></code></a>.</p>\n<div class=\"versionadded\">\n<p><span class=\"versionmodified added\">Added in version 3.2.</span></p>\n</div>\n</dd></dl>\n\n<dl class=\"py exception\">\n<dt class=\"sig sig-object py\" id=\"tokenize.TokenError\">\n<em class=\"property\"><span class=\"k\"><span class=\"pre\">exception</span></span><span class=\"w\"> </span></em><span class=\"sig-prename descclassname\"><span class=\"pre\">tokenize.</span></span><span class=\"sig-name descname\"><span class=\"pre\">TokenError</span></span><a class=\"headerlink\" href=\"#tokenize.TokenError\" title=\"Link to this definition\">¶</a></dt>\n<dd><p>Raised when either a docstring or expression that may be split over several\nlines is not completed anywhere in the file, for example:</p>\n<div class=\"highlight-python3 notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"s2\">&quot;&quot;&quot;Beginning of</span>\n<span class=\"s2\">docstring</span>\n</pre></div>\n</div>\n<p>or:</p>\n<div class=\"highlight-python3 notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span>\n <span class=\"mi\">2</span><span class=\"p\">,</span>\n <span class=\"mi\">3</span>\n</pre></div>\n</div>\n</dd></dl>\n\n</section>\n<section id=\"command-line-usage\">\n<span id=\"tokenize-cli\"></span><h2>Command-Line Usage<a class=\"headerlink\" href=\"#command-line-usage\" title=\"Link to this heading\">¶</a></h2>\n<div class=\"versionadded\">\n<p><span class=\"versionmodified added\">Added in version 3.3.</span></p>\n</div>\n<p>The <a class=\"reference internal\" href=\"#module-tokenize\" title=\"tokenize: Lexical scanner for Python source code.\"><code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">tokenize</span></code></a> module can be executed as a script from the command line.\nIt is as simple as:</p>\n<div class=\"highlight-sh notranslate\"><div class=\"highlight\"><pre><span></span>python<span class=\"w\"> </span>-m<span class=\"w\"> </span>tokenize<span class=\"w\"> </span><span class=\"o\">[</span>-e<span class=\"o\">]</span><span class=\"w\"> </span><span class=\"o\">[</span>filename.py<span class=\"o\">]</span>\n</pre></div>\n</div>\n<p>The following options are accepted:</p>\n<dl class=\"std option\">\n<dt class=\"sig sig-object std\" id=\"cmdoption-tokenize-h\">\n<span id=\"cmdoption-tokenize-help\"></span><span class=\"sig-name descname\"><span class=\"pre\">-h</span></span><span class=\"sig-prename descclassname\"></span><span class=\"sig-prename descclassname\"><span class=\"pre\">,</span> </span><span class=\"sig-name descname\"><span class=\"pre\">--help</span></span><span class=\"sig-prename descclassname\"></span><a class=\"headerlink\" href=\"#cmdoption-tokenize-h\" title=\"Link to this definition\">¶</a></dt>\n<dd><p>show this help message and exit</p>\n</dd></dl>\n\n<dl class=\"std option\">\n<dt class=\"sig sig-object std\" id=\"cmdoption-tokenize-e\">\n<span id=\"cmdoption-tokenize-exact\"></span><span class=\"sig-name descname\"><span class=\"pre\">-e</span></span><span class=\"sig-prename descclassname\"></span><span class=\"sig-prename descclassname\"><span class=\"pre\">,</span> </span><span class=\"sig-name descname\"><span class=\"pre\">--exact</span></span><span class=\"sig-prename descclassname\"></span><a class=\"headerlink\" href=\"#cmdoption-tokenize-e\" title=\"Link to this definition\">¶</a></dt>\n<dd><p>display token names using the exact type</p>\n</dd></dl>\n\n<p>If <code class=\"file docutils literal notranslate\"><span class=\"pre\">filename.py</span></code> is specified its contents are tokenized to stdout.\nOtherwise, tokenization is performed on stdin.</p>\n</section>\n<section id=\"examples\">\n<h2>Examples<a class=\"headerlink\" href=\"#examples\" title=\"Link to this heading\">¶</a></h2>\n<p>Example of a script rewriter that transforms float literals into Decimal\nobjects:</p>\n<div class=\"highlight-python3 notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">tokenize</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">tokenize</span><span class=\"p\">,</span> <span class=\"n\">untokenize</span><span class=\"p\">,</span> <span class=\"n\">NUMBER</span><span class=\"p\">,</span> <span class=\"n\">STRING</span><span class=\"p\">,</span> <span class=\"n\">NAME</span><span class=\"p\">,</span> <span class=\"n\">OP</span>\n<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">io</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">BytesIO</span>\n\n<span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">decistmt</span><span class=\"p\">(</span><span class=\"n\">s</span><span class=\"p\">):</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;Substitute Decimals for floats in a string of statements.</span>\n\n<span class=\"sd\">    &gt;&gt;&gt; from decimal import Decimal</span>\n<span class=\"sd\">    &gt;&gt;&gt; s = &#39;print(+21.3e-5*-.1234/81.7)&#39;</span>\n<span class=\"sd\">    &gt;&gt;&gt; decistmt(s)</span>\n<span class=\"sd\">    &quot;print (+Decimal (&#39;21.3e-5&#39;)*-Decimal (&#39;.1234&#39;)/Decimal (&#39;81.7&#39;))&quot;</span>\n\n<span class=\"sd\">    The format of the exponent is inherited from the platform C library.</span>\n<span class=\"sd\">    Known cases are &quot;e-007&quot; (Windows) and &quot;e-07&quot; (not Windows).  Since</span>\n<span class=\"sd\">    we&#39;re only showing 12 digits, and the 13th isn&#39;t close to 5, the</span>\n<span class=\"sd\">    rest of the output should be platform-independent.</span>\n\n<span class=\"sd\">    &gt;&gt;&gt; exec(s)  #doctest: +ELLIPSIS</span>\n<span class=\"sd\">    -3.21716034272e-0...7</span>\n\n<span class=\"sd\">    Output from calculations with Decimal should be identical across all</span>\n<span class=\"sd\">    platforms.</span>\n\n<span class=\"sd\">    &gt;&gt;&gt; exec(decistmt(s))</span>\n<span class=\"sd\">    -3.217160342717258261933904529E-7</span>\n<span class=\"sd\">    &quot;&quot;&quot;</span>\n    <span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n    <span class=\"n\">g</span> <span class=\"o\">=</span> <span class=\"n\">tokenize</span><span class=\"p\">(</span><span class=\"n\">BytesIO</span><span class=\"p\">(</span><span class=\"n\">s</span><span class=\"o\">.</span><span class=\"n\">encode</span><span class=\"p\">(</span><span class=\"s1\">&#39;utf-8&#39;</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">readline</span><span class=\"p\">)</span>  <span class=\"c1\"># tokenize the string</span>\n    <span class=\"k\">for</span> <span class=\"n\">toknum</span><span class=\"p\">,</span> <span class=\"n\">tokval</span><span class=\"p\">,</span> <span class=\"n\">_</span><span class=\"p\">,</span> <span class=\"n\">_</span><span class=\"p\">,</span> <span class=\"n\">_</span> <span class=\"ow\">in</span> <span class=\"n\">g</span><span class=\"p\">:</span>\n        <span class=\"k\">if</span> <span class=\"n\">toknum</span> <span class=\"o\">==</span> <span class=\"n\">NUMBER</span> <span class=\"ow\">and</span> <span class=\"s1\">&#39;.&#39;</span> <span class=\"ow\">in</span> <span class=\"n\">tokval</span><span class=\"p\">:</span>  <span class=\"c1\"># replace NUMBER tokens</span>\n            <span class=\"n\">result</span><span class=\"o\">.</span><span class=\"n\">extend</span><span class=\"p\">([</span>\n                <span class=\"p\">(</span><span class=\"n\">NAME</span><span class=\"p\">,</span> <span class=\"s1\">&#39;Decimal&#39;</span><span class=\"p\">),</span>\n                <span class=\"p\">(</span><span class=\"n\">OP</span><span class=\"p\">,</span> <span class=\"s1\">&#39;(&#39;</span><span class=\"p\">),</span>\n                <span class=\"p\">(</span><span class=\"n\">STRING</span><span class=\"p\">,</span> <span class=\"nb\">repr</span><span class=\"p\">(</span><span class=\"n\">tokval</span><span class=\"p\">)),</span>\n                <span class=\"p\">(</span><span class=\"n\">OP</span><span class=\"p\">,</span> <span class=\"s1\">&#39;)&#39;</span><span class=\"p\">)</span>\n            <span class=\"p\">])</span>\n        <span class=\"k\">else</span><span class=\"p\">:</span>\n            <span class=\"n\">result</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">((</span><span class=\"n\">toknum</span><span class=\"p\">,</span> <span class=\"n\">tokval</span><span class=\"p\">))</span>\n    <span class=\"k\">return</span> <span class=\"n\">untokenize</span><span class=\"p\">(</span><span class=\"n\">result</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">decode</span><span class=\"p\">(</span><span class=\"s1\">&#39;utf-8&#39;</span><span class=\"p\">)</span>\n</pre></div>\n</div>\n<p>Example of tokenizing from the command line.  The script:</p>\n<div class=\"highlight-python3 notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">say_hello</span><span class=\"p\">():</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">&quot;Hello, World!&quot;</span><span class=\"p\">)</span>\n\n<span class=\"n\">say_hello</span><span class=\"p\">()</span>\n</pre></div>\n</div>\n<p>will be tokenized to the following output where the first column is the range\nof the line/column coordinates where the token is found, the second column is\nthe name of the token, and the final column is the value of the token (if any)</p>\n<div class=\"highlight-shell-session notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">$ </span>python<span class=\"w\"> </span>-m<span class=\"w\"> </span>tokenize<span class=\"w\"> </span>hello.py\n<span class=\"go\">0,0-0,0:            ENCODING       &#39;utf-8&#39;</span>\n<span class=\"go\">1,0-1,3:            NAME           &#39;def&#39;</span>\n<span class=\"go\">1,4-1,13:           NAME           &#39;say_hello&#39;</span>\n<span class=\"go\">1,13-1,14:          OP             &#39;(&#39;</span>\n<span class=\"go\">1,14-1,15:          OP             &#39;)&#39;</span>\n<span class=\"go\">1,15-1,16:          OP             &#39;:&#39;</span>\n<span class=\"go\">1,16-1,17:          NEWLINE        &#39;\\n&#39;</span>\n<span class=\"go\">2,0-2,4:            INDENT         &#39;    &#39;</span>\n<span class=\"go\">2,4-2,9:            NAME           &#39;print&#39;</span>\n<span class=\"go\">2,9-2,10:           OP             &#39;(&#39;</span>\n<span class=\"go\">2,10-2,25:          STRING         &#39;&quot;Hello, World!&quot;&#39;</span>\n<span class=\"go\">2,25-2,26:          OP             &#39;)&#39;</span>\n<span class=\"go\">2,26-2,27:          NEWLINE        &#39;\\n&#39;</span>\n<span class=\"go\">3,0-3,1:            NL             &#39;\\n&#39;</span>\n<span class=\"go\">4,0-4,0:            DEDENT         &#39;&#39;</span>\n<span class=\"go\">4,0-4,9:            NAME           &#39;say_hello&#39;</span>\n<span class=\"go\">4,9-4,10:           OP             &#39;(&#39;</span>\n<span class=\"go\">4,10-4,11:          OP             &#39;)&#39;</span>\n<span class=\"go\">4,11-4,12:          NEWLINE        &#39;\\n&#39;</span>\n<span class=\"go\">5,0-5,0:            ENDMARKER      &#39;&#39;</span>\n</pre></div>\n</div>\n<p>The exact token type names can be displayed using the <a class=\"reference internal\" href=\"#cmdoption-tokenize-e\"><code class=\"xref std std-option docutils literal notranslate\"><span class=\"pre\">-e</span></code></a> option:</p>\n<div class=\"highlight-shell-session notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">$ </span>python<span class=\"w\"> </span>-m<span class=\"w\"> </span>tokenize<span class=\"w\"> </span>-e<span class=\"w\"> </span>hello.py\n<span class=\"go\">0,0-0,0:            ENCODING       &#39;utf-8&#39;</span>\n<span class=\"go\">1,0-1,3:            NAME           &#39;def&#39;</span>\n<span class=\"go\">1,4-1,13:           NAME           &#39;say_hello&#39;</span>\n<span class=\"go\">1,13-1,14:          LPAR           &#39;(&#39;</span>\n<span class=\"go\">1,14-1,15:          RPAR           &#39;)&#39;</span>\n<span class=\"go\">1,15-1,16:          COLON          &#39;:&#39;</span>\n<span class=\"go\">1,16-1,17:          NEWLINE        &#39;\\n&#39;</span>\n<span class=\"go\">2,0-2,4:            INDENT         &#39;    &#39;</span>\n<span class=\"go\">2,4-2,9:            NAME           &#39;print&#39;</span>\n<span class=\"go\">2,9-2,10:           LPAR           &#39;(&#39;</span>\n<span class=\"go\">2,10-2,25:          STRING         &#39;&quot;Hello, World!&quot;&#39;</span>\n<span class=\"go\">2,25-2,26:          RPAR           &#39;)&#39;</span>\n<span class=\"go\">2,26-2,27:          NEWLINE        &#39;\\n&#39;</span>\n<span class=\"go\">3,0-3,1:            NL             &#39;\\n&#39;</span>\n<span class=\"go\">4,0-4,0:            DEDENT         &#39;&#39;</span>\n<span class=\"go\">4,0-4,9:            NAME           &#39;say_hello&#39;</span>\n<span class=\"go\">4,9-4,10:           LPAR           &#39;(&#39;</span>\n<span class=\"go\">4,10-4,11:          RPAR           &#39;)&#39;</span>\n<span class=\"go\">4,11-4,12:          NEWLINE        &#39;\\n&#39;</span>\n<span class=\"go\">5,0-5,0:            ENDMARKER      &#39;&#39;</span>\n</pre></div>\n</div>\n<p>Example of tokenizing a file programmatically, reading unicode\nstrings instead of bytes with <a class=\"reference internal\" href=\"#tokenize.generate_tokens\" title=\"tokenize.generate_tokens\"><code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">generate_tokens()</span></code></a>:</p>\n<div class=\"highlight-python3 notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">tokenize</span>\n\n<span class=\"k\">with</span> <span class=\"n\">tokenize</span><span class=\"o\">.</span><span class=\"n\">open</span><span class=\"p\">(</span><span class=\"s1\">&#39;hello.py&#39;</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">f</span><span class=\"p\">:</span>\n    <span class=\"n\">tokens</span> <span class=\"o\">=</span> <span class=\"n\">tokenize</span><span class=\"o\">.</span><span class=\"n\">generate_tokens</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"o\">.</span><span class=\"n\">readline</span><span class=\"p\">)</span>\n    <span class=\"k\">for</span> <span class=\"n\">token</span> <span class=\"ow\">in</span> <span class=\"n\">tokens</span><span class=\"p\">:</span>\n        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">token</span><span class=\"p\">)</span>\n</pre></div>\n</div>\n<p>Or reading bytes directly with <a class=\"reference internal\" href=\"#tokenize.tokenize\" title=\"tokenize.tokenize\"><code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">tokenize()</span></code></a>:</p>\n<div class=\"highlight-python3 notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">tokenize</span>\n\n<span class=\"k\">with</span> <span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"s1\">&#39;hello.py&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;rb&#39;</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">f</span><span class=\"p\">:</span>\n    <span class=\"n\">tokens</span> <span class=\"o\">=</span> <span class=\"n\">tokenize</span><span class=\"o\">.</span><span class=\"n\">tokenize</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"o\">.</span><span class=\"n\">readline</span><span class=\"p\">)</span>\n    <span class=\"k\">for</span> <span class=\"n\">token</span> <span class=\"ow\">in</span> <span class=\"n\">tokens</span><span class=\"p\">:</span>\n        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">token</span><span class=\"p\">)</span>\n</pre></div>\n</div>\n</section>\n</section>\n\n\n            <div class=\"clearer\"></div>\n          </div>\n        </div>\n      </div>\n      <div class=\"sphinxsidebar\" role=\"navigation\" aria-label=\"Main\">\n        <div class=\"sphinxsidebarwrapper\">\n  <div>\n    <h3><a href=\"../contents.html\">Table of Contents</a></h3>\n    <ul>\n<li><a class=\"reference internal\" href=\"#\"><code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">tokenize</span></code> — Tokenizer for Python source</a><ul>\n<li><a class=\"reference internal\" href=\"#tokenizing-input\">Tokenizing Input</a></li>\n<li><a class=\"reference internal\" href=\"#command-line-usage\">Command-Line Usage</a></li>\n<li><a class=\"reference internal\" href=\"#examples\">Examples</a></li>\n</ul>\n</li>\n</ul>\n\n  </div>\n  <div>\n    <h4>Previous topic</h4>\n    <p class=\"topless\"><a href=\"keyword.html\"\n                          title=\"previous chapter\"><code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">keyword</span></code> — Testing for Python keywords</a></p>\n  </div>\n  <div>\n    <h4>Next topic</h4>\n    <p class=\"topless\"><a href=\"tabnanny.html\"\n                          title=\"next chapter\"><code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">tabnanny</span></code> — Detection of ambiguous indentation</a></p>\n  </div>\n  <div role=\"note\" aria-label=\"source link\">\n    <h3>This page</h3>\n    <ul class=\"this-page-menu\">\n      <li><a href=\"../bugs.html\">Report a bug</a></li>\n      <li>\n        <a href=\"https://github.com/python/cpython/blob/main/Doc/library/tokenize.rst?plain=1\"\n            rel=\"nofollow\">Show source\n        </a>\n      </li>\n    </ul>\n  </div>\n        </div>\n<div id=\"sidebarbutton\" title=\"Collapse sidebar\">\n<span>«</span>\n</div>\n\n      </div>\n      <div class=\"clearer\"></div>\n    </div>  \n    <div class=\"related\" role=\"navigation\" aria-label=\"Related\">\n      <h3>Navigation</h3>\n      <ul>\n        <li class=\"right\" style=\"margin-right: 10px\">\n          <a href=\"../genindex.html\" title=\"General Index\"\n             >index</a></li>\n        <li class=\"right\" >\n          <a href=\"../py-modindex.html\" title=\"Python Module Index\"\n             >modules</a> |</li>\n        <li class=\"right\" >\n          <a href=\"tabnanny.html\" title=\"tabnanny — Detection of ambiguous indentation\"\n             >next</a> |</li>\n        <li class=\"right\" >\n          <a href=\"keyword.html\" title=\"keyword — Testing for Python keywords\"\n             >previous</a> |</li>\n\n          <li><img src=\"../_static/py.svg\" alt=\"Python logo\" style=\"vertical-align: middle; margin-top: -1px\"></li>\n          <li><a href=\"https://www.python.org/\">Python</a> &#187;</li>\n          <li class=\"switchers\">\n            <div class=\"language_switcher_placeholder\"></div>\n            <div class=\"version_switcher_placeholder\"></div>\n          </li>\n          <li>\n              \n          </li>\n    <li id=\"cpython-language-and-version\">\n      <a href=\"../index.html\">3.14.0 Documentation</a> &#187;\n    </li>\n\n          <li class=\"nav-item nav-item-1\"><a href=\"index.html\" >The Python Standard Library</a> &#187;</li>\n          <li class=\"nav-item nav-item-2\"><a href=\"language.html\" >Python Language Services</a> &#187;</li>\n        <li class=\"nav-item nav-item-this\"><a href=\"\"><code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">tokenize</span></code> — Tokenizer for Python source</a></li>\n                <li class=\"right\">\n                    \n\n    <div class=\"inline-search\" role=\"search\">\n        <form class=\"inline-search\" action=\"../search.html\" method=\"get\">\n          <input placeholder=\"Quick search\" aria-label=\"Quick search\" type=\"search\" name=\"q\" id=\"search-box\">\n          <input type=\"submit\" value=\"Go\">\n        </form>\n    </div>\n                     |\n                </li>\n            <li class=\"right\">\n<label class=\"theme-selector-label\">\n    Theme\n    <select class=\"theme-selector\" oninput=\"activateTheme(this.value)\">\n        <option value=\"auto\" selected>Auto</option>\n        <option value=\"light\">Light</option>\n        <option value=\"dark\">Dark</option>\n    </select>\n</label> |</li>\n            \n      </ul>\n    </div>  \n    <div class=\"footer\">\n    &copy; <a href=\"../copyright.html\">Copyright</a> 2001 Python Software Foundation.\n    <br>\n    This page is licensed under the Python Software Foundation License Version 2.\n    <br>\n    Examples, recipes, and other code in the documentation are additionally licensed under the Zero Clause BSD License.\n    <br>\n    \n      See <a href=\"/license.html\">History and License</a> for more information.<br>\n    \n    \n    <br>\n\n    The Python Software Foundation is a non-profit corporation.\n<a href=\"https://www.python.org/psf/donations/\">Please donate.</a>\n<br>\n    <br>\n      Last updated on Nov 25, 2025 (18:01 UTC).\n    \n      <a href=\"/bugs.html\">Found a bug</a>?\n    \n    <br>\n\n    Created using <a href=\"https://www.sphinx-doc.org/\">Sphinx</a> 8.2.3.\n    </div>\n\n    <script type=\"text/javascript\" src=\"../_static/switchers.js\"></script>\n  </body>\n</html>",
  "metadata": {
    "title": "tokenize — Tokenizer for Python source — Python 3.14.0 documentation",
    "meta_description": "Source code: Lib/tokenize.py The tokenize module provides a lexical scanner for Python source code, implemented in Python. The scanner in this module returns comments as tokens as well, making it u...",
    "h1": "— Tokenizer for Python source"
  },
  "request_headers": {
    "Referer": "https://docs.python.org/3/library/",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "en",
    "User-Agent": "DumbCrawler/1.0",
    "Accept-Encoding": "gzip, deflate"
  },
  "response_headers": {
    "Content-Length": "8934",
    "Server": "nginx",
    "Content-Type": "text/html",
    "Last-Modified": "Tue, 25 Nov 2025 18:03:26 GMT",
    "Etag": "\"6925ef6e-adfd\"",
    "X-Clacks-Overhead": "GNU Terry Pratchett",
    "Strict-Transport-Security": "max-age=315360000; includeSubDomains; preload",
    "Via": "1.1 varnish, 1.1 varnish",
    "Accept-Ranges": "bytes",
    "Age": "3750",
    "Date": "Wed, 26 Nov 2025 05:32:28 GMT",
    "X-Served-By": "cache-lga21939-LGA, cache-yul1970070-YUL",
    "X-Cache": "HIT, HIT",
    "X-Cache-Hits": "239, 0",
    "X-Timer": "S1764135148.346414,VS0,VE1",
    "Vary": "Accept-Encoding"
  },
  "crawled_at": "2025-11-26T05:32:30.004529+00:00",
  "crawler": {
    "name": "DumbCrawler",
    "version": "1.0",
    "mode": "crawl",
    "js_mode": "off",
    "scope": "subfolder"
  }
}